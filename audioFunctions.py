# audioFunctions.py - Audio processing con CUDA optimizado (CORREGIDO)
import warnings
import importlib
from functools import lru_cache
from typing import Union, List, Any
import numpy as np
from numpy.typing import NDArray
import time
import torch
import sounddevice as sd
from threading import Lock

# COM initialization (Windows)
try:
    import pythoncom
    try:
        pythoncom.CoInitialize()
    except Exception:
        pass
except Exception:
    pass

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CUDA Setup
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

device = "cuda" if torch.cuda.is_available() else "cpu"

if device == "cuda":
    # Optimizaciones CUDA para RTX 2060
    torch.backends.cudnn.benchmark = True       # Autotuning de kernels
    torch.backends.cuda.matmul.allow_tf32 = True # TF32 para matmul (m√°s r√°pido, ~misma precisi√≥n)
    torch.backends.cudnn.allow_tf32 = True
    torch.set_float32_matmul_precision('high')   # Permite TF32
    print(f"üöÄ CUDA activo: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    # Optimizaciones CPU
    torch.set_num_threads(4)
    torch.set_num_interop_threads(2)
    torch.set_flush_denormal(True)
    print("‚ö†Ô∏è  CUDA no disponible, usando CPU")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Model Manager (Whisper)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class ModelManager:
    _instance = None
    _whisper_model: Any = None
    _lock = Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    @property
    def whisper(self) -> Any:
        if self._whisper_model is None:
            with self._lock:
                if self._whisper_model is None:
                    print("‚è≥ Cargando Whisper...")
                    import whisper
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore", category=FutureWarning)
                        # RTX 2060 (6GB): large-v2 en CUDA, small en CPU
                        if device == "cuda":
                            model_size = "large-v2"  # ~3GB VRAM
                        else:
                            model_size = "small"
                        self._whisper_model = whisper.load_model(
                            model_size,
                            device=device
                        )
                    print(f"‚úÖ Whisper cargado ({model_size} en {device.upper()})")
        return self._whisper_model

    def unload_whisper(self) -> None:
        if self._whisper_model is not None:
            with self._lock:
                if self._whisper_model is not None:
                    del self._whisper_model
                    self._whisper_model = None
                    if device == "cuda":
                        torch.cuda.empty_cache()
                    print("üóëÔ∏è  Whisper descargado")


model_manager = ModelManager()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Whisper Transcription con CUDA
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def whisperTranscription(audio_data: NDArray[np.int16], language: str = 'es') -> str:
    """Transcripci√≥n con Whisper, optimizada para CUDA"""
    try:
        if len(audio_data) == 0:
            return ""

        # Normalizar a float32
        audio_float = audio_data.astype(np.float32) / 32768.0

        # Saltar audio demasiado corto (< 0.5s)
        if len(audio_float) < 22050:
            return ""

        # Resample de 44100 ‚Üí 16000 Hz (requerido por Whisper)
        from scipy import signal as scipy_signal
        orig_sr, target_sr = 44100, 16000
        n_samples = int(len(audio_float) * target_sr / orig_sr)
        
        # scipy.signal.resample puede devolver tuple o ndarray dependiendo de la versi√≥n
        resampled_result = scipy_signal.resample(audio_float, n_samples)
        
        # Manejar ambos casos
        if isinstance(resampled_result, tuple):
            resampled = resampled_result[0]  # Tomar primer elemento si es tuple
        else:
            resampled = resampled_result

        # Asegurar que sea 1D
        if resampled.ndim > 1:
            resampled = resampled.mean(axis=1)

        model = model_manager.whisper

        result = model.transcribe(
            resampled,
            language=language,
            fp16=(device == "cuda"),       # FP16 en GPU = 2x m√°s r√°pido
            verbose=False,
            condition_on_previous_text=False,
            compression_ratio_threshold=2.4,
            no_speech_threshold=0.6,
            beam_size=3,                   # Reducido para m√°s velocidad
            best_of=3,
            task="transcribe"
        )

        # result['text'] puede ser str o dict dependiendo de la versi√≥n de whisper
        text_result = result.get('text', '') if isinstance(result, dict) else str(result)
        text = text_result.strip() if isinstance(text_result, str) else ""
        
        if text:
            print(f"üìù User: {text}")
        return text

    except Exception as e:
        print(f"‚ùå Transcription error: {e}")
        return ""


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Audio Recording
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def recordAudio(silence_duration: float = 1.5,
                volume_threshold: float = 12,
                max_duration: float = 20) -> NDArray[np.int16]:
    """Graba audio con detecci√≥n de silencio adaptativa"""
    from collections import deque

    fs = 44100
    chunk_size = 1024
    started_recording = False
    silence_start = None
    buffer: deque = deque(maxlen=int(0.8 * fs))
    full_recording: List[NDArray] = []  # Sin especificar dtype para evitar conflictos
    start_time = time.time()
    energy_history: deque = deque(maxlen=30)
    adaptive_threshold = volume_threshold

    try:
        with sd.InputStream(samplerate=fs, channels=1,
                            dtype=np.float32, blocksize=chunk_size) as stream:
            while True:
                if time.time() - start_time > max_duration:
                    print("‚è±Ô∏è  Duraci√≥n m√°xima alcanzada")
                    break

                audio_chunk, _ = stream.read(chunk_size)
                volume = np.sqrt(np.mean(np.square(audio_chunk))) * 1000

                # Umbral adaptativo
                energy_history.append(volume)
                if len(energy_history) == 30:
                    avg_noise = np.mean(list(energy_history)[:15])
                    adaptive_threshold = max(volume_threshold, avg_noise * 1.5)

                buffer.append(audio_chunk)

                if not started_recording:
                    print("*Escuchando...*", end='\r', flush=True)
                    if volume > adaptive_threshold:
                        print("*Grabando...*", end='\r', flush=True)
                        full_recording = list(buffer)
                        started_recording = True
                        silence_start = None
                else:
                    full_recording.append(audio_chunk)
                    if volume < adaptive_threshold * 0.7:
                        if silence_start is None:
                            silence_start = time.time()
                        elif (time.time() - silence_start) > silence_duration:
                            print("*Procesando...*")
                            break
                    else:
                        silence_start = None

        if full_recording:
            combined = np.concatenate(full_recording)
            max_val = np.max(np.abs(combined))
            if max_val > 0:
                normalized = combined / max_val * 32767
                return normalized.astype(np.int16)
            return (combined * 32767).astype(np.int16)

        return np.array([], dtype=np.int16)

    except Exception as e:
        print(f"\n‚ùå Recording error: {e}")
        return np.array([], dtype=np.int16)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# TTS (Coqui-TTS) con CUDA
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@lru_cache(maxsize=1)
def get_tts_model() -> Any:
    """Carga el modelo TTS con CUDA si est√° disponible"""
    try:
        from TTS.api import TTS
    except ImportError:
        raise RuntimeError("coqui-tts no instalado: pip install coqui-tts")

    # Safe globals para torch.load - versi√≥n corregida
    candidates = [
        "TTS.config.shared_configs.BaseDatasetConfig",
        "TTS.tts.configs.xtts_config.XttsConfig",
        "TTS.tts.configs.xtts_config.XttsAudioConfig",
        "TTS.tts.models.xtts.XttsArgs",
        "TTS.tts.models.xtts.XttsAudioConfig",
        "TTS.tts.layers.xtts.tokenizer.VoiceBpeTokenizer",
    ]

    safe_globals: List[Any] = []
    for path in candidates:
        mod_path, _, cls_name = path.rpartition('.')
        try:
            mod = importlib.import_module(mod_path)
            cls = getattr(mod, cls_name)
            safe_globals.append(cls)
        except Exception:
            pass

    if safe_globals:
        try:
            # Verificar si add_safe_globals existe
            if hasattr(torch.serialization, 'add_safe_globals'):
                torch.serialization.add_safe_globals(safe_globals)  # type: ignore[attr-defined]
        except Exception:
            pass

    print(f"‚è≥ Cargando TTS en {device.upper()}...")
    use_gpu = (device == "cuda")
    tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=use_gpu)
    print(f"‚úÖ TTS cargado en {device.upper()}")
    return tts


def generateAudio(text: str, speaker_file: str,
                  language: str = "es", sample_rate: int = 24000) -> bool:
    """Genera y reproduce audio TTS"""
    if not text or not text.strip():
        return False

    # Limitar longitud para evitar latencia excesiva
    if len(text) > 400:
        text = text[:397] + "..."

    try:
        tts = get_tts_model()

        # Generar audio (la inferencia corre en CUDA autom√°ticamente)
        wav = tts.tts(text=text, speaker_wav=speaker_file, language=language)

        if wav is None or len(wav) == 0:
            print("‚ùå TTS devolvi√≥ audio vac√≠o")
            return False

        sd.play(wav, sample_rate)
        sd.wait()
        return True

    except Exception as e:
        print(f"‚ùå TTS error: {e}")
        return False


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# VRAM Monitor (√∫til para debug con RTX 2060)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def print_cuda_stats() -> None:
    """Muestra uso de VRAM"""
    if device != "cuda":
        return
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    reserved  = torch.cuda.memory_reserved(0) / 1024**3
    total     = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"üñ•Ô∏è  VRAM: {allocated:.2f}GB usado / {reserved:.2f}GB reservado / {total:.1f}GB total")